# Generated Outputs from Transformer Seq2Seq
PS D:\LLM_Experiments> cd transformer-seq2seq                                     
PS D:\LLM_Experiments\transformer-seq2seq> python train.py                        
Training on 10 samples with vocab size 74...                                      
Epoch 20, Loss: 0.0281
Epoch 40, Loss: 0.0098
Epoch 60, Loss: 0.0052
Epoch 80, Loss: 0.0032
Epoch 100, Loss: 0.0022
Epoch 120, Loss: 0.0016
Epoch 140, Loss: 0.0012
Epoch 160, Loss: 0.0010
Epoch 180, Loss: 0.0008
Epoch 200, Loss: 0.0006
Training Complete. Model and Vocab Saved.
PS D:\LLM_Experiments\transformer-seq2seq> python inference.py                    
Model Loaded. Vocabulary Size: 74                                                 

--- Inference Results ---
Input : AI improves healthcare
Output: AI enhances medical diagnosis and treatment
------------------------------
Input : Transformers process data in parallel
Output: Transformers handle sequences simultaneously
------------------------------
Input : What is self-attention?
Output: Self-attention relates each word to every other word
------------------------------
Input : Why is positional encoding required?
Output: Positional encoding provides word order information
------------------------------
Input : In the future, AI will
Output: In the future, AI will automate decision systems
------------------------------
PS D:\LLM_Experiments\transformer-seq2seq> 
## Model Verification
The following outputs were generated by the trained Transformer model.

---

**Input**: AI improves healthcare
**Output**: AI enhances medical diagnosis and treatment

**Input**: Transformers process data in parallel
**Output**: Transformers handle sequences simultaneously

**Input**: What is self-attention?
**Output**: Self-attention relates each word to every other word

**Input**: Why is positional encoding required?
**Output**: Positional encoding provides word order information

**Input**: In the future, AI will
**Output**: In the future, AI will automate decision systems

---
*Note: The model was trained to overfit these specific examples for demonstration purposes.*
